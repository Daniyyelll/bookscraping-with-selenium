{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a56761",
   "metadata": {},
   "source": [
    "# Scraping Childerns Book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1f5e2",
   "metadata": {},
   "source": [
    "## Project Introduction\n",
    "\n",
    "This project focuses on web scraping using **Selenium** to extract data on children’s books from the fictional online bookstore <a href=\"https://books.toscrape.com/\">Books to Scrape</a>. The goal of the project is to demonstrate practical skills in *web automation*, and *data collection and extraction*.\n",
    "\n",
    "The scraped data is then organized and stored into **JSON** and **CSV** files.\n",
    "\n",
    "This project highlights the ability to:\n",
    "\n",
    "- Navigate and interact with websites programmatically.\n",
    "\n",
    "- Handle page structures and categories.\n",
    "\n",
    "- Extract and structure relevant information efficiently.\n",
    "\n",
    "- Apply automation techniques for real-world data collection scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405cc7e",
   "metadata": {},
   "source": [
    "Preparing Selenium toolkit and the main url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c789c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57964efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5042d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"https://toscrape.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fbd6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(main_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183e0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bookstore_link = driver.find_element(By.PARTIAL_LINK_TEXT, \"bookstore\").get_attribute(\n",
    "    \"href\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "455394fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com/\n"
     ]
    }
   ],
   "source": [
    "print(bookstore_link)  # Verifyin the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d59460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigating to the bookstore page\n",
    "driver.find_element(By.PARTIAL_LINK_TEXT, \"bookstore\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f010ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category URL: https://books.toscrape.com/catalogue/category/books/childrens_11/index.html\n"
     ]
    }
   ],
   "source": [
    "# Verifying links and category\n",
    "category = driver.find_element(By.LINK_TEXT, \"Childrens\")\n",
    "category_url = category.get_attribute(\"href\")\n",
    "print(f\"Category URL: {category_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addeea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "category.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3ac62dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Name: Childrens\n",
      "Category Results: 29 results - showing 1 to 20.\n"
     ]
    }
   ],
   "source": [
    "category_name = (\n",
    "    driver.find_element(By.CLASS_NAME, \"page-header\")\n",
    "    .find_element(By.TAG_NAME, \"h1\")\n",
    "    .text\n",
    ")\n",
    "category_results = driver.find_element(By.CSS_SELECTOR, \"form.form-horizontal\").text\n",
    "\n",
    "print(f\"Category Name: {category_name}\")\n",
    "print(f\"Category Results: {category_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928d152",
   "metadata": {},
   "source": [
    "We concluded that there is pagination for our desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0be6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagination = True\n",
    "column = [\"Upc\", \"Title\", \"Price\", \"Rating\", \"Stock\", \"Stock_Qty\", \"Url\", \"Image\"]\n",
    "data_set = []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7406cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1 -- 1\n",
      "Data -- Birdsong: A Story in Pictures -- star-rating Three -- price\n",
      "Data -- The Bear and the Piano -- star-rating One -- price\n",
      "Data -- The Secret of Dreadwillow Carse -- star-rating One -- price\n",
      "Data -- The White Cat and the Monk: A Retelling of the Poem “Pangur Bán” -- star-rating Four -- price\n",
      "Data -- Little Red -- star-rating Three -- price\n",
      "Data -- Walt Disney's Alice in Wonderland -- star-rating Five -- price\n",
      "Data -- Twenty Yawns -- star-rating Two -- price\n",
      "Data -- Rain Fish -- star-rating Three -- price\n",
      "Data -- Once Was a Time -- star-rating Two -- price\n",
      "Data -- Luis Paints the World -- star-rating Three -- price\n",
      "Data -- Nap-a-Roo -- star-rating One -- price\n",
      "Data -- The Whale -- star-rating Four -- price\n",
      "Data -- Shrunken Treasures: Literary Classics, Short, Sweet, and Silly -- star-rating Three -- price\n",
      "Data -- Raymie Nightingale -- star-rating Two -- price\n",
      "Data -- Playing from the Heart -- star-rating One -- price\n",
      "Data -- Maybe Something Beautiful: How Art Transformed a Neighborhood -- star-rating One -- price\n",
      "Data -- The Wild Robot -- star-rating Three -- price\n",
      "Data -- The Thing About Jellyfish -- star-rating One -- price\n",
      "Data -- The Lonely Ones -- star-rating Five -- price\n",
      "Data -- The Day the Crayons Came Home (Crayons) -- star-rating Five -- price\n",
      "Processing page 2 -- 21\n",
      "Data -- The Cat in the Hat (Beginner Books B-1) -- star-rating Two -- price\n",
      "Data -- Red: The True Story of Red Riding Hood -- star-rating Three -- price\n",
      "Data -- Horrible Bear! -- star-rating Two -- price\n",
      "Data -- Green Eggs and Ham (Beginner Books B-16) -- star-rating Four -- price\n",
      "Data -- Counting Thyme -- star-rating One -- price\n",
      "Data -- Are We There Yet? -- star-rating Three -- price\n",
      "Data -- Diary of a Minecraft Zombie Book 1: A Scare of a Dare (An Unofficial Minecraft Book) -- star-rating Four -- price\n",
      "Data -- Matilda -- star-rating One -- price\n",
      "Data -- Charlie and the Chocolate Factory (Charlie Bucket #1) -- star-rating Three -- price\n",
      "No more pagination or cannot reach it, currently at page 2\n"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "while pagination:\n",
    "    try:\n",
    "        print(f\"Processing page {page} -- {count + 1}\")\n",
    "        listings = driver.find_elements(By.CSS_SELECTOR, \"ol.row li\")\n",
    "\n",
    "        for listing in listings:\n",
    "            article = listing.find_element(By.TAG_NAME, \"article\")\n",
    "\n",
    "            image = article.find_element(By.CSS_SELECTOR, \"a\")\n",
    "            article_link = image.get_attribute(\"href\")\n",
    "            image_src = image.find_element(By.TAG_NAME, \"img\").get_attribute(\"src\")\n",
    "            image_alt = image.find_element(By.TAG_NAME, \"img\").get_attribute(\"alt\")\n",
    "\n",
    "            rating = article.find_element(\n",
    "                By.CSS_SELECTOR, 'p[class*=\"star\"]'\n",
    "            ).get_attribute(\"class\")\n",
    "            title = article.find_element(By.CSS_SELECTOR, \"h3 > a\").get_attribute(\n",
    "                \"title\"\n",
    "            )\n",
    "            price = article.find_element(By.CLASS_NAME, \"price_color\").text\n",
    "\n",
    "            # printing extracted data for following\n",
    "            print(f\"Data -- {title} -- {rating} -- price\")\n",
    "\n",
    "            # Introduce a short delay between requests to mimic human behavior and prevent IP blocking.\n",
    "            time.sleep(1)\n",
    "\n",
    "            if article_link:\n",
    "                listing.find_element(By.TAG_NAME, \"img\").click()\n",
    "                upc = driver.find_element(\n",
    "                    By.XPATH, '//th[contains(text(),\"UPC\")]/following-sibling::td'\n",
    "                ).text\n",
    "\n",
    "                if upc:\n",
    "                    stock_qty = driver.find_element(\n",
    "                        By.XPATH,\n",
    "                        '//th[contains(text(), \"Availability\")]/following-sibling::td',\n",
    "                    ).text\n",
    "\n",
    "                    stock = stock_qty.split(\"(\")\n",
    "\n",
    "                    temp = [\n",
    "                        upc,\n",
    "                        title,\n",
    "                        price,\n",
    "                        rating.replace(\"star-rating\", \"\").strip(),\n",
    "                        stock[0].strip(),\n",
    "                        stock[1].replace(\"avalable\", \"\").replace(\")\", \"\").strip(),\n",
    "                        article_link,\n",
    "                        image_src,\n",
    "                    ]\n",
    "                count += 1\n",
    "                data_set.append(temp)\n",
    "\n",
    "            # Add a delay to avoid being flagged as a bot.\n",
    "            time.sleep(2)\n",
    "            # Go back to the listing page\n",
    "            driver.back()\n",
    "        try:\n",
    "            driver.find_element(By.LINK_TEXT, \"next\").click()\n",
    "            page += 1\n",
    "        except NoSuchElementException:\n",
    "            pagination = False\n",
    "            print(f\"No more pagination or cannot reach it, currently at page {page}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception Occured: {str(e)}\")\n",
    "        pagination = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a3aa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 29\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows in dataset: {len(data_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbee51b",
   "metadata": {},
   "source": [
    "Exporting the scraped data into csv and json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf5a0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947e320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created\n",
      "JSON file created\n"
     ]
    }
   ],
   "source": [
    "with open(\"book_details.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column)\n",
    "    for data in data_set:\n",
    "        writer.writerow(data)\n",
    "    print(\"CSV file created\")\n",
    "\n",
    "final_data_set = list()\n",
    "for data in data_set:\n",
    "    final_data_set.append(dict(zip(column, data)))\n",
    "\n",
    "with open(\"book_details.json\", \"w\") as json_file:\n",
    "    json.dump(final_data_set, json_file, indent=4)\n",
    "\n",
    "print(\"JSON file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b3315",
   "metadata": {},
   "source": [
    "quitting the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "151be7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119641a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully demonstrated the use of Selenium as a powerful tool for automating web scraping tasks in a structured and efficient manner.\n",
    "\n",
    "- By carefully verifying navigations, validating URLs, and incorporating pagination handling, the scraper was able to traverse through multiple pages of the Children’s Books category without data loss.\n",
    "\n",
    "- Both **CSS selectors** and **XPath expressions** were utilized to accurately locate and extract relevant elements from the DOM, ensuring flexibility and robustness in the scraping process. \n",
    "\n",
    "- To mimic natural browsing behavior and reduce the likelihood of being flagged as a bot, **time.sleep** delays were strategically introduced between requests, imitating human interaction with the website.\n",
    "\n",
    "Finally, the collected data was exported efficiently into **JSON** and **CSV** formats, enabling easy storage, sharing, and further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
